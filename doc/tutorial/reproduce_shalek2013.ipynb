{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      ".. _reproduce_shalek2013:\n",
      "\n",
      ".. currentmodule:: flotilla"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Shalek and Satija, *et al* (2013)"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "In the interest of reproducibility, and to showcase our new package [`flotilla`](http://github.com/yeolab/flotilla), I've reproduced many figures from the landmark single-cell paper, [Single-cell transcriptomics reveals bimodality in expression and splicing in immune cells](http://www.ncbi.nlm.nih.gov/pubmed/23685454) by Shalek and Satija, *et al*. *Nature* (2013). In this paper, Regev and colleagues performed single-cell sequencing 18 bone marrow-derived dendritic cells (BMDCs), in addition to 3 pooled samples.\n",
      "\n",
      ".. note::\n",
      "    \n",
      "    This tutorial covers the dataset exploration, **not** the downloading, cleaning, and importing. For those steps, please refer to the `\"**Loading data from Shalek and Satija (2013)**\" <getting_started_shalek2013>`_ tutorial.\n",
      "\n",
      "Before we begin, let's import everything we need."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Turn on inline plots with IPython\n",
      "%matplotlib inline\n",
      "\n",
      "# Import the flotilla package for biological data analysis\n",
      "import flotilla\n",
      "\n",
      "# Import \"numerical python\" library for number crunching\n",
      "import numpy as np\n",
      "\n",
      "# Import \"panel data analysis\" library for tabular data\n",
      "import pandas as pd\n",
      "\n",
      "# Import statistical data visualization package\n",
      "# Note: As of November 6th, 2014, you will need the \"master\" version of \n",
      "# seaborn on github (v0.5.dev), installed via \n",
      "# \"pip install git+ssh://git@github.com/mwaskom/seaborn.git\n",
      "import seaborn as sns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "First, we'll load the data that's been prepared:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "study = flotilla.embark(flotilla._shalek2013)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "To expose what's happening here, the ``embark`` command is loading data from a ``datapackage.json`` file as described at this location:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "flotilla._shalek2013"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "The `datapackage.json` file is what holds all the information relative to the study, and loosely follows the [datapackage spec](http://data.okfn.org/doc/data-package) created by the Open Knowledge Foundation.\n",
      "\n",
      "The data it's getting downloaded into ``$HOME/flotilla_projects/<study_name>/`` (the ``$HOME`` means my \"home directory\", which on my laptop is ``/Users/olga``). This will be saved in your home directory, too. We can take a look at the downloaded ``datapackage.json`` file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cat $HOME/flotilla_projects/shalek2013/datapackage.json"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Let's look at what else is in this folder:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ls /Users/olga/flotilla_projects/shalek2013"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "So this is where all the other files are. Good to know!\n",
      "\n",
      "Now we can start creating figures!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Figure 1"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "\n",
      "Here, we will attempt to re-create the sub-panels in `Figure 1 <http://www.nature.com/nature/journal/v498/n7453/fig_tab/nature12172_F1.html>`, where the original is:\n",
      "\n",
      ".. image:: http://www.nature.com/nature/journal/v498/n7453/images/nature12172-f1.2.jpg\n",
      "    :alt: Original Figure 1"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Figure 1a"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "study.plot_two_samples('P1', 'P2')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Figure 1b"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Paper: $r=0.54$. Not sure at all what's going on here."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "study.plot_two_samples('S1', 'S2')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "By the way, you can do other kinds of plots with ``flotilla``, like a `kernel density estimate <http://en.wikipedia.org/wiki/Kernel_density_estimation>`_ (\"``kde``\") plot:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "study.plot_two_samples('S1', 'S2', kind='kde')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Or a binned hexagon plot (\"``hexbin``\"):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "study.plot_two_samples('S1', 'S2', kind='hexbin')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Any inputs that are valid to ``seaborn``'s ```jointplot`` <http://web.stanford.edu/~mwaskom/software/seaborn/generated/seaborn.jointplot.html#seaborn.jointplot>`_ are valid."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Figure 1c"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = study.expression.data.ix['P1']\n",
      "y = study.expression.singles.mean()\n",
      "y.name = \"Average singles\"\n",
      "\n",
      "jointgrid = sns.jointplot(x, y, joint_kws=dict(alpha=0.5))\n",
      "\n",
      "# Adjust xmin, ymin to 0\n",
      "xmin, xmax, ymin, ymax = jointgrid.ax_joint.axis()\n",
      "jointgrid.ax_joint.set_xlim(0, xmax)\n",
      "jointgrid.ax_joint.set_ylim(0, ymax);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Figure 2"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Next, we will attempt to recreate the figures from `Figure 2 <http://www.nature.com/nature/journal/v498/n7453/fig_tab/nature12172_F2.html>`_:\n",
      "\n",
      ".. image:: http://www.nature.com/nature/journal/v498/n7453/images/nature12172-f2.2.jpg\n",
      "    :alt: Original Figure 2"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Figure 2a"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "For this figure, we will need the \"LPS Response\" and \"Housekeeping\" gene annotations, from the ``expression_feature_data`` that we created."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get colors for plotting the gene categories\n",
      "dark2 = sns.color_palette('Dark2')\n",
      "\n",
      "singles = study.expression.singles\n",
      "# Get only gene categories for genes in the singles data\n",
      "singles, gene_categories = singles.align(study.expression.feature_data['Gene Category'], join='left', axis=1)\n",
      "\n",
      "mean = singles.mean()\n",
      "std = singles.std()\n",
      "\n",
      "jointgrid = sns.jointplot(mean, std, color='#262626', joint_kws=dict(alpha=0.5))\n",
      "\n",
      "for i, (category, s) in enumerate(gene_categories.groupby(gene_categories)):\n",
      "    jointgrid.ax_joint.plot(mean[s.index], std[s.index], 'o', color=dark2[i], markersize=5)\n",
      "\n",
      "jointgrid.ax_joint.set_xlabel('Standard deviation in single cells $\\mu$')\n",
      "jointgrid.ax_joint.set_ylabel('Average expression in single cells $\\sigma$')\n",
      "\n",
      "xmin, xmax, ymin, ymax = jointgrid.ax_joint.axis()\n",
      "vmax = max(xmax, ymax)\n",
      "vmin = min(xmin, ymin)\n",
      "jointgrid.ax_joint.plot([0, vmax], [0, vmax], color='steelblue')\n",
      "jointgrid.ax_joint.plot([0, vmax], [0, .25*vmax], color='grey')\n",
      "jointgrid.ax_joint.set_xlim(0, xmax)\n",
      "jointgrid.ax_joint.set_ylim(0, ymax)\n",
      "\n",
      "jointgrid.ax_joint.fill_betweenx((ymin, ymax), 0, np.log(250), alpha=0.5, zorder=-1);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "I couldn't find the data for the ``hESC``s for the right-side panel of Fig. 2a, so I couldn't remake the figure."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Figure 2b"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "In the paper, they use *\"522 most highly expressed genes (single-cell average TPM > 250)\"*, but I wasn't able to replicate their numbers. If I use the pre-filtered expression data that I fed into flotilla, then I get 297 genes:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean = study.expression.singles.mean()\n",
      "highly_expressed_genes = mean.index[mean > np.log(250 + 1)]\n",
      "len(highly_expressed_genes)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Which is much less. If I use the original, unfiltered data, then I get the *\"522\"* number, but this seems strange because they did the filtering step of *\"discarded genes not appreciably expressed (transcripts per million (TPM) > 1) in at least three individual cells, retaining 6,313 genes for further analysis\"*, and yet they went back to the original data to get this new subset.\n",
      "\n",
      "To get their original expression matrix, we use `wget` and the filename from GEO."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "wget ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE41nnn/GSE41265/suppl/GSE41265_allGenesTPM.txt.gz"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Then, we read in this file using `pandas`, transpose it, and get only the rows from the single cells. You can read more about reading in this data in the `\"**Loading data from Shalek and Satija (2013)**\" <getting_started_shalek2013>`_ tutorial."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "expression = pd.read_table(\"GSE41265_allGenesTPM.txt.gz\", compression=\"gzip\", index_col=0)\n",
      "expression = expression.T\n",
      "singles_ids = study.expression.singles.index\n",
      "\n",
      "mean = expression.ix[singles_ids].mean()\n",
      "highly_expressed_genes = mean.index[mean > 250]\n",
      "len(highly_expressed_genes)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "expression_highly_expressed = np.log(expression.ix[singles_ids, expression.ix[singles_ids].mean() > 250] + 1)\n",
      "\n",
      "mean = expression_highly_expressed.mean()\n",
      "\n",
      "std = expression_highly_expressed.std()\n",
      "\n",
      "mean_bins = pd.cut(mean, bins=np.arange(0, 11, 1))\n",
      "\n",
      "# Coefficient of variation\n",
      "cv = std/mean\n",
      "cv.sort()\n",
      "\n",
      "genes = mean.index\n",
      "\n",
      "\n",
      "# for name, df in shalek2013.expression.singles.groupby(dict(zip(genes, mean_bins)), axis=1):\n",
      "def calculate_cells_per_tpm_per_cv(df, cv):\n",
      "    df = df[df > 1]\n",
      "    df_aligned, cv_aligned = df.align(cv, join='inner', axis=1)\n",
      "    cv_aligned.sort()\n",
      "    n_cells = pd.Series(0, index=cv.index)\n",
      "    n_cells[cv_aligned.index] = df_aligned.ix[:, cv_aligned.index].count()\n",
      "    return n_cells\n",
      "\n",
      "grouped = expression_highly_expressed.groupby(dict(zip(genes, mean_bins)), axis=1)\n",
      "cells_per_tpm_per_cv = grouped.apply(calculate_cells_per_tpm_per_cv, cv=cv)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here's how you would make the original figure from the paper:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig, ax = plt.subplots(figsize=(10, 10))\n",
      "sns.heatmap(cells_per_tpm_per_cv, linewidth=0, ax=ax, yticklabels=False)\n",
      "ax.set_yticks([])\n",
      "ax.set_xlabel('ln(TPM, binned)');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Doesn't quite look the same. Maybe the y-axis labels were opposite, and higher up on the y-axis was less variant? Because I see a blob of color for (1,2] TPM (by the way, the figure in the paper is not TPM+1 as previous figures were)\n",
      "\n",
      "This is how you would make a modified version of the figure, which also plots the coefficient of variation on a side-plot, which I like because it shows the CV changes directly on the heatmap. Also, technically this is $\\ln$(TPM+1)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from matplotlib import gridspec\n",
      "\n",
      "fig = plt.figure(figsize=(12, 10))\n",
      "\n",
      "gs = gridspec.GridSpec(1, 2, wspace=0.01, hspace=0.01, width_ratios=[.2, 1])\n",
      "cv_ax = fig.add_subplot(gs[0, 0])\n",
      "heatmap_ax = fig.add_subplot(gs[0, 1])\n",
      "\n",
      "sns.heatmap(cells_per_tpm_per_cv, linewidth=0, ax=heatmap_ax)\n",
      "heatmap_ax.set_yticks([])\n",
      "heatmap_ax.set_xlabel('$\\ln$(TPM+1), binned')\n",
      "\n",
      "y = np.arange(cv.shape[0])\n",
      "cv_ax.set_xscale('log')\n",
      "cv_ax.plot(cv, y, color='#262626')\n",
      "cv_ax.fill_betweenx(cv, np.zeros(cv.shape), y, color='#262626', alpha=0.5)\n",
      "cv_ax.set_ylim(0, y.max())\n",
      "cv_ax.set_xlabel('CV = $\\mu/\\sigma$')\n",
      "cv_ax.set_yticks([])\n",
      "sns.despine(ax=cv_ax, left=True, right=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Figure 3"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "We will attempt to re-create the sub-panel figures from [Figure 3](http://www.nature.com/nature/journal/v498/n7453/fig_tab/nature12172_F3.html):\n",
      "\n",
      "![Original Figure 3](http://www.nature.com/nature/journal/v498/n7453/images/nature12172-f3.2.jpg)\n",
      "\n",
      "Since we can't re-do the microscopy (Figure 3a) or the RNA-FISH counts (Figure 3c), we will make Figures 3b. These histograms are simple to do outside of `flotilla`, so we do not have them within flotilla."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Figure 3b, top panel"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots()\n",
      "sns.distplot(study.splicing.singles.values.flat, bins=np.arange(0, 1.05, 0.05), ax=ax)\n",
      "ax.set_xlim(0, 1)\n",
      "sns.despine()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Figure 3b, bottom panel"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots()\n",
      "sns.distplot(study.splicing.pooled.values.flat, bins=np.arange(0, 1.05, 0.05), ax=ax, color='grey')\n",
      "ax.set_xlim(0, 1)\n",
      "sns.despine()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Figure 4"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "We will attempt to re-create the sub-panel figures from [Figure 4](http://www.nature.com/nature/journal/v498/n7453/fig_tab/nature12172_F4.html):\n",
      "\n",
      "![Original Figure 4](http://www.nature.com/nature/journal/v498/n7453/images/nature12172-f4.2.jpg)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Figure 4a"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Here, we can use the \"`interactive_pca`\" function we have to explore different dimensionality reductions on the data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A \"sequences shortened\" version of this is available as a gif:\n",
      "\n",
      "![Imgur](http://i.imgur.com/fJKPQ7W.gif)\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Equivalently, I could have written out the plotting command by hand, instead of using `study.interactive_pca`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "study.plot_pca(feature_subset='Gene Category: LPS Response', sample_subset='not (pooled)', plot_violins=False, show_point_labels=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Mark immature cells as a new subset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As in the paper, the cells S12, S13, and S16 appear in a cluster that is separate from the remaining cells. From the paper, these were the \"matured\" bone-marrow derived dendritic cells, after stimulation with a lipopolysaccharide. We can mark these as mature in our metadata,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mature = ['S12', 'S13', 'S16']\n",
      "study.metadata.data['maturity'] = metadata.index.map(lambda x: 'mature' if x in mature else 'immature')\n",
      "study.metadata.data.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then, we can set **maturity** as the column we use for coloring the PCA, since before it was the \"phenotype\" column."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "study.metadata.phenotype_col = 'maturity'\n",
      "study.save('shalek2013')\n",
      "study = flotilla.embark('shalek2013')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "study.plot_pca(feature_subset='Gene Category: LPS Response', sample_subset='not (pooled)', plot_violins=False, show_point_labels=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "study.save('shalek2013')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Figure 4b"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lps_response_genes = study.expression.feature_subsets['Gene Category: LPS Response']\n",
      "lps_response = study.expression.singles.ix[:, lps_response_genes].dropna(how='all', axis=1)\n",
      "lps_response.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lps_response_corr = lps_response.corr()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "\"Elbow method\" for determining number of clusters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The authors state that they used the \"Elbow method\" to determine the [number of cluster centers](http://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set). Essentially, you try a bunch of different $k$, and see where there is a flattening out of the metric, like an elbow. There's a few different variations on which metric to use, such as using the average distance to the cluster center, or the explained variance. Let's try the distance to cluster center first, because `scikit-learn` makes it easy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cluster import KMeans\n",
      "\n",
      "##### cluster data into K=1..10 clusters #####\n",
      "ks = np.arange(1, 11).astype(int)\n",
      "\n",
      "X = lps_response_corr.values\n",
      "\n",
      "kmeans = [KMeans(n_clusters=k).fit(X) for k in ks]\n",
      "\n",
      "# Scikit-learn makes this easy by computing the distance to the nearest center\n",
      "dist_to_center = [km.inertia_ for km in kmeans]\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.plot(ks, dist_to_center, 'o-')\n",
      "ax.set_ylabel('Sum of distance to nearest cluster center')\n",
      "sns.despine()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Not quite sure where the elbow is here. looks like there's a big drop off after $k=1$, but that could just be an illusion. Since they didn't specify which version of the elbow method they used, I'm not going to investigate this further, and just see if we can see what they see with the $k=5$ clusters that they found was optimal.\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kmeans = KMeans(n_clusters=5)\n",
      "lps_response_corr_clusters = kmeans.fit_predict(lps_response_corr.values)\n",
      "lps_response_corr_clusters"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's create a dataframe with these genes in their cluster orders."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gene_to_cluster = dict(zip(lps_response_corr.columns, lps_response_corr_clusters))\n",
      "\n",
      "dfs = []\n",
      "for name, df in lps_response_corr.groupby(gene_to_cluster):\n",
      "    dfs.append(df)\n",
      "lps_response_corr_ordered_by_clusters = pd.concat(dfs)\n",
      "\n",
      "# Make symmetric, since we created this dataframe by smashing rows on top of each other, we need to reorder the columns\n",
      "lps_response_corr_ordered_by_clusters = lps_response_corr_ordered_by_clusters.ix[:, lps_response_corr_ordered_by_clusters.index]\n",
      "lps_response_corr_ordered_by_clusters.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The next step is to get the principal-component reduced data, using only the LPS response genes. We can do this in `flotilla` using `study.expression.reduce`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reduced = study.expression.reduce(singles_ids, feature_ids=lps_response_genes)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can get the principal components using `reduced.components_` (similar interface as `scikit-learn`)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reduced.components_.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pc_components = reduced.components_.ix[:2, lps_response_corr_ordered_by_clusters.index].T\n",
      "pc_components.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib as mpl\n",
      "\n",
      "fig = plt.figure(figsize=(12, 10))\n",
      "gs = gridspec.GridSpec(2, 2, wspace=0.1, hspace=0.1, width_ratios=[1, .2], height_ratios=[1, .1])\n",
      "corr_ax = fig.add_subplot(gs[0, 0])\n",
      "corr_cbar_ax = fig.add_subplot(gs[1, 0])\n",
      "pc_ax = fig.add_subplot(gs[0, 1:])\n",
      "pc_cbar_ax = fig.add_subplot(gs[1:, 1:])\n",
      "\n",
      "sns.heatmap(lps_response_corr_ordered_by_clusters, linewidth=0, ax=corr_ax, cbar_ax=corr_cbar_ax, \n",
      "            cbar_kws=dict(orientation='horizontal'))\n",
      "sns.heatmap(pc_components, cmap=mpl.cm.PRGn, linewidth=0, ax=pc_ax, cbar_ax=pc_cbar_ax,\n",
      "            cbar_kws=dict(orientation='horizontal'))\n",
      "\n",
      "corr_ax.set_xlabel('')\n",
      "corr_ax.set_ylabel('')\n",
      "corr_ax.set_xticks([])\n",
      "corr_ax.set_yticks([])\n",
      "pc_ax.set_yticks([])\n",
      "pc_ax.set_ylabel('')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This looks pretty similar, maybe just rearranged cluster order. Let's check what their data looks like when you plot this."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Their PC scores and clusters for the genes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gene_pc_clusters = pd.read_excel('nature12172-s1/Supplementary_Table5.xls', index_col=0)\n",
      "gene_pc_clusters.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = lps_response_corr.ix[gene_pc_clusters.index, gene_pc_clusters.index].dropna(how='all', axis=0).dropna(how='all', axis=1)\n",
      "\n",
      "fig = plt.figure(figsize=(12, 10))\n",
      "gs = gridspec.GridSpec(2, 2, wspace=0.1, hspace=0.1, width_ratios=[1, .2], height_ratios=[1, .1])\n",
      "corr_ax = fig.add_subplot(gs[0, 0])\n",
      "corr_cbar_ax = fig.add_subplot(gs[1, 0])\n",
      "pc_ax = fig.add_subplot(gs[0, 1:])\n",
      "pc_cbar_ax = fig.add_subplot(gs[1:, 1:])\n",
      "\n",
      "sns.heatmap(data, linewidth=0, square=True, vmin=-1, vmax=1, ax=corr_ax, cbar_ax=corr_cbar_ax, cbar_kws=dict(orientation='horizontal'))\n",
      "sns.heatmap(gene_pc_clusters.ix[:, ['PC1 Score', 'PC2 Score']], linewidth=0, cmap=mpl.cm.PRGn,\n",
      "            ax=pc_ax, cbar_ax=pc_cbar_ax, cbar_kws=dict(orientation='horizontal'), xticklabels=False, yticklabels=False)\n",
      "\n",
      "corr_ax.set_xlabel('')\n",
      "corr_ax.set_ylabel('')\n",
      "corr_ax.set_xticks([])\n",
      "corr_ax.set_yticks([])\n",
      "\n",
      "pc_ax.set_yticks([])\n",
      "pc_ax.set_ylabel('');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sure enough, if I use their annotations, I get exactly that. Though there were two genes in their file that I didn't have in the `lps_response_corr` data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gene_pc_clusters.index.difference(lps_response_corr.index)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Oh joy, another `datetime` error, just like we had with `expression2`... Looking back at the original Excel file, there is one gene that Excel mangled to be a date:\n",
      "\n",
      "![](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_supplementary_table_5_datetime_error.png)\n",
      "\n",
      "Please, can we start using just plain ole `.csv`s for supplementary data! Excel does NOT preserve strings if they start with numbers, and instead thinks they are dates."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import collections\n",
      "collections.Counter(gene_pc_clusters.index.map(type))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Yep, it's just that one that got mangled.... oh well."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gene_pc_clusters_genes = set(filter(lambda x: isinstance(x, unicode), gene_pc_clusters.index))\n",
      "gene_pc_clusters_genes.difference(lps_response_corr.index)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, \"`RPS6KA2`\" is the only gene that was in their list of genes and not in mine."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Supplementary figures"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we get to have even more fun by plotting the Supplementary figures! :D\n",
      "\n",
      "Ironically, the supplementary figures are usually way easier to access (like not behind a paywall), and yet they're usually the documents that really have the crucial information about the experiments."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Supplementary Figure 1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![Supplementary figure 1, a correlation plot](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig1.png)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "singles_mean = study.expression.singles.mean()\n",
      "singles_mean.name = 'Single cell average'\n",
      "\n",
      "# Need to convert \"average_singles\" to a DataFrame instead of a single-row Series\n",
      "singles_mean = pd.DataFrame(singles_mean)\n",
      "singles_mean.head()\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_for_correlations = pd.concat([study.expression.singles, singles_mean.T, study.expression.pooled])\n",
      "\n",
      "# Take the transpose of the data, because the plotting algorithm calculates correlations between columns,\n",
      "# And we want the correlations between samples, not features\n",
      "data_for_correlations = data_for_correlations.T\n",
      "data_for_correlations.head()\n",
      "\n",
      "# %time sns.corrplot(data_for_correlations)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(figsize=(10, 10))\n",
      "sns.corrplot(data_for_correlations, ax=ax)\n",
      "sns.despine()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that this is mostly red, while in the figure from the paper, it was both blue and red. This is because the colormap started at 0.2 (not negative), and was centered with white at about 0.6. I see that they're trying to emphasize how much more correlated the pooled samples are to each other, but I think a simple sequential map would have been more effective."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Supplementary Figures 2 and 3"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Supplementary [Figure 2](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig2.png) and [Figure 3](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig3.png) are from FISH and raw sequence data, and are out of the scope of this computational reproduction."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Supplementary Figure 4"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Supplementary Figure 4](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig4.png) was from published data, however the citation in the Supplementary Information (#23) was a [machine-learning book](http://link.springer.com/book/10.1007%2F978-3-642-51175-2), and #23 in the main text citations was a [review of probabilistic graphical models](http://www.sciencemag.org/content/303/5659/799.full), neither of which have the mouse embryonic stem cells or mouse embryonic fibroblasts used in the figure.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Supplementary Figure 5"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For this figure, we can only plot 5d, since it's derived directly from a table in their dataset.\n",
      "\n",
      "Warning: these data are going to require some serious cleaning. Yay data janitorial duties!\n",
      "\n",
      "![](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig5.png)\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Supplementary Figure 5d"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "barcoded = pd.read_excel('nature12172-s1/Supplementary_Table7.xlsx')\n",
      "barcoded.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first three columns are TPM calculated from the three samples that have molecular barcodes, and the last three columns are the integer counts of molecular barcodes from the three molecular barcode samples.\n",
      "\n",
      "Let's remove the \"Unnamed: 3\" column which is all NaNs. We'll do that with the `.dropna` method, specifying `axis=1` for columns and `how=\"all\"` to make sure only columns that have ALL NaNs are removed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "barcoded = barcoded.dropna(how='all', axis=1)\n",
      "barcoded.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, let's drop that pesky \"GENE\" row. Don't worry, we'll get the sample ID names back next."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "barcoded = barcoded.drop('GENE', axis=0)\n",
      "barcoded.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll create a `pandas.MultiIndex` from the tuples of `(sample_id, measurement_type)` pair."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "columns = pd.MultiIndex.from_tuples([('MB_S1', 'TPM'),\n",
      "           ('MB_S2', 'TPM'),\n",
      "           ('MB_S3', 'TPM'),\n",
      "           ('MB_S1', 'Unique Barcodes'),\n",
      "           ('MB_S2', 'Unique Barcodes'),\n",
      "           ('MB_S3', 'Unique Barcodes')])\n",
      "barcoded.columns = columns\n",
      "barcoded = barcoded.sort_index(axis=1)\n",
      "barcoded.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the next move, we're going to do some crazy `pandas`-fu. First we're going to transpose, then `reset_index` of the transpose. Just so you know what this looks like, it's this."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "barcoded.T.reset_index().head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we're going to transform the data into a [tidy](http://vita.had.co.nz/papers/tidy-data.pdf) format, with separate columns for sample ids, measurement types, the gene that was measured, and its measurement value."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "barcoded_tidy = pd.melt(barcoded.T.reset_index(), id_vars=['level_0', 'level_1'])\n",
      "barcoded_tidy.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's rename these columns into something more useful, instead of \"level_0\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "barcoded_tidy = barcoded_tidy.rename(columns={'level_0': 'sample_id', 'level_1': 'measurement', 'variable': 'gene_name'})\n",
      "barcoded_tidy.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we're going to take some seemingly-duplicating steps, but trust me, it'll make the data easier."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "barcoded_tidy['TPM'] = barcoded_tidy.value[barcoded_tidy.measurement == 'TPM']\n",
      "barcoded_tidy['Unique Barcodes'] = barcoded_tidy.value[barcoded_tidy.measurement == 'Unique Barcodes']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fill the values of the \"**TPM**\"'s forwards, since they appear first, and fill the values of the \"**Unique Barcodes**\" backwards, since they're second"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "barcoded_tidy.TPM = barcoded_tidy.TPM.ffill()\n",
      "barcoded_tidy['Unique Barcodes'] = barcoded_tidy['Unique Barcodes'].bfill()\n",
      "barcoded_tidy.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Drop the \"**measurement**\" column and drop duplicate rows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "barcoded_tidy = barcoded_tidy.drop('measurement', axis=1)\n",
      "barcoded_tidy = barcoded_tidy.drop_duplicates()\n",
      "barcoded_tidy.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "barcoded_tidy['log TPM'] = np.log(barcoded_tidy.TPM)\n",
      "barcoded_tidy['log Unique Barcodes'] = np.log(barcoded_tidy['Unique Barcodes'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can use the convenient linear model plot (`lmplot`) in `seaborn` to plot these three samples together!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sns.lmplot('log TPM', 'log Unique Barcodes', barcoded_tidy, col='sample_id')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Supplementary Figures 6-20"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Supplementary Figures [6](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig6.png), \n",
      "[7](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig7.png),\n",
      "[8](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig8.png), \n",
      "[9](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig9.png), \n",
      "[10](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig10.png), \n",
      "[11](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig11.png), \n",
      "[12](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig12.png), \n",
      "[13](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig13.png), \n",
      "[14](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig14.png), \n",
      "[15](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig15.png), \n",
      "[16](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig16.png), \n",
      "[17](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig17.png), \n",
      "[18](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig18.png), \n",
      "[19](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig19.png), and\n",
      "[20](https://raw.githubusercontent.com/olgabot/olgabot.github.io-source/master/content/images/shalek2013_sfig20.png), \n",
      "deal with splicing data from the molecular barcodes, RNA-FISH, flow-sorted cells, and single-cell RT-PCR and are out of the scope of this reproduction."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Conclusions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While there may be minor, undocumented, differences between the methods presented in the manuscript and the figures, the application of [`flotilla`](https://github.com/YeoLab/flotilla) presents an opportunity to avoid these types of inconsistencies by strictly documenting every change to code and every transformation of the data. The biology the authors found is clearly real, as they did the knockout experiment of *Ifnr-/-* and saw that indeed the maturation process was affected, and *Stat2* and *Irf7* had much lower expression, as with the \"maturing\" cells in the data."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}
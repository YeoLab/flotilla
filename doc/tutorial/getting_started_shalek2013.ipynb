{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. _getting_started_shalek2013:\n",
    "\n",
    ".. currentmodule:: flotilla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data from Shalek and Satija (2013)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the 2013 paper, `Single-cell transcriptomics reveals bimodality in expression and splicing in immune cells <http://www.ncbi.nlm.nih.gov/pubmed/23685454>`_ (Shalek and Satija, *et al*. *Nature* (2013)), Regev and colleagues performed single-cell sequencing 18 bone marrow-derived dendritic cells (BMDCs), in addition to 3 pooled samples.\n",
    "\n",
    "This will show you the nitty-gritty of what files you need and in what format to get started with using ```flotilla`` <http://yeolab.github.io/flotilla/docs>`_. In addition to being a tutorial explaining ``flotilla`` inputs, the goal of this document is also to expose every individual step of reformatting data.\n",
    "\n",
    "We will go over:\n",
    "\n",
    "* Using ``wget`` to download data over the internet from the command line\n",
    "* Using ``pandas`` to read in Excel tables and comma-separated variable (``csv``) files\n",
    "* Basic orientation to working with ``pandas``\n",
    "    * Transposing matrix data\n",
    "    * Subsetting data on rows and columns\n",
    "    * Accessing a single column\n",
    "    * Changing the values of a single column\n",
    "* Cleaning data using ``pandas`` and Python standard library packages\n",
    "    * Removing genes with mangled IDs\n",
    "    * Fixing inconsistent sample naming between data tables\n",
    "    * Removing NAs\n",
    "* Importing the data into ``flotilla``\n",
    "\n",
    "Before we begin, let's import everything we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the flotilla package for biological data analysis\n",
    "import flotilla\n",
    "\n",
    "# Import \"numerical python\" library for number crunching\n",
    "import numpy as np\n",
    "\n",
    "# Import \"panel data analysis\" library for tabular data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expression data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "First, we will read in the expression data. These data were obtained using the command ``wget`` to download files from the internet via the command line, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE41nnn/GSE41265/suppl/GSE41265_allGenesTPM.txt.gz"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We will also compare to the supplementary table 2 data, obtained also using ``wget``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget http://www.nature.com/nature/journal/v498/n7453/extref/nature12172-s1.zip\n",
    "unzip nature12172-s1.zip"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we will read the data in using ```pandas`` <http://pandas.pydata.org>`_, indicating that the file is compressed via gzip, and that we want the first column to be the \"index\", or row names of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expression = pd.read_table(\"GSE41265_allGenesTPM.txt.gz\", compression=\"gzip\", index_col=0)\n",
    "expression.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "These data are in the \"transcripts per million,\" aka TPM unit. See `this blog post <http://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/>`_ if that sounds weird to you.\n",
    "\n",
    "These data are formatted with samples on the columns, and genes on the rows. But we want the opposite, with samples on the rows and genes on the columns. This follows ```scikit-learn`` <http://scikit-learn.org/stable/tutorial/basic/tutorial.html#loading-an-example-dataset>`_'s standard of data matrices with size (``n_samples``, ``n_features``) as each gene is a feature. So we will simply transpose this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expression = expression.T\n",
    "expression.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The authors filtered the expression data based on having at least 3 single cells express genes with at TPM (transcripts per million, ) > 1. We can express this in using the ``pandas`` DataFrames easily.\n",
    "\n",
    "First, from reading the paper and looking at the data, I know there are 18 single cells, and there are 18 samples that start with the letter \"S.\" So I will extract the single samples from the ``index`` (row names) using a ``lambda``, a tiny function which in this case, tells me whether or not that sample id begins with the letter \"S\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "singles_ids = expression.index[expression.index.map(lambda x: x.startswith('S'))]\n",
    "print('number of single cells:', len(singles_ids))\n",
    "singles = expression.ix[singles_ids]\n",
    "\n",
    "expression_filtered = expression.ix[:, singles[singles > 1].count() >= 3]\n",
    "expression_filtered = np.log(expression_filtered + 1)\n",
    "expression_filtered.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hmm, that's strange. The paper states that they had 6313 genes after filtering, but I get 6312. Even using \"`singles >= 1`\" doesn't help. I suppose it's one of the usual bioinformatics 1-off errors :)\n",
    "\n",
    "(I also tried this with the expression table provided in the supplementary data as \"`SupplementaryTable2.xlsx`,\" and got the same results.)\n",
    "\n",
    "Now that we've taken care of importing and filtering the expression data, let's do the feature data of the expression data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expression feature data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The \"Expresion feature data\" is similar to the ``fData`` from ``BioconductoR``, where there's some additional data on your features that you want to look at. They uploaded information about the features in their OTHER expression matrix, uploaded as a supplementary file, ``Supplementary_Table2.xlsx``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expression2 = pd.read_excel('nature12172-s1/Supplementary_Table2.xlsx')\n",
    "\n",
    "# # This was also in features x samples format, so we need to transpose\n",
    "# expression2 = expression2.T\n",
    "# expression2.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Unfortunately, Excel mangled the gene names of the genes that started with a ``201``, and converted those into ``datetime.datetime`` objects, or if the gene ID was all numbers, then it was converted to an integer. We could go in to excel save the file as a csv which would fix this, but for now we'll just ignore those rows.\n",
    "\n",
    "See, not all the items in the ``GENE`` column are ``unicode`` string objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set([type(x) for x in expression2.GENE])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "One is an integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set([x for x in expression2.GENE if isinstance(x, int)])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Several more were converted to ``datetime`` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "set([x for x in expression2.GENE if isinstance(x, datetime.datetime)])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To ignore those columns, we'll only use items in the ``GENE`` column if they are of the type ``unicode``, as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expression2.GENE = expression2.GENE[expression2.GENE.map(lambda x: isinstance(x, unicode))]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can take a look at a subset and see that row 25's gene was replaced with an NaN, indicating that the gene there wasn't a string and didn't pass our filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expression2.ix[20:25, :5]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can remove all the rows with an NaN using ``dropna`` and indicating we only want the function to pay attention to a ``subset`` of the columns. Let's check the ``shape`` of the dataframe so we can see if there were tons of columns removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expression2.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let's look at the 20th to 25th rows again to see if the NaN gene is still there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expression2 = expression2.dropna(subset=['GENE'])\n",
    "expression2.ix[20:25, :5]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Nope, not there! How many columns did we end up removing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expression2.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Not many, only 27 total genes. Shouldn't have a HUGE impact on the analysis. It's not like we know what those genes do anyway...\n",
    "\n",
    "Now let's get down to doing what we came here to do: making a table of the feature data. We'll use the column \"Gene Category\" in this dataframe, which we can see is in the last column by subsetting on the first five rows with ``:5`` and the last five columns with ``-5:``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expression2.ix[:5, -5:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can subset ``expression2`` on the two columns we want: the gene ID in ``GENE`` and the gene category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gene_category = expression2[['GENE', 'Gene Category']]\n",
    "gene_category.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We want to create a ``pandas.DataFrame`` from the \"Gene Category\" row for our ``expression_feature_data``, which we will do via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expression_feature_data = gene_category.set_index('GENE')\n",
    "expression_feature_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splicing Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We obtain the splicing data from this study from the supplementary information, specifically the file, ``Supplementary_Table4.xls``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splicing = pd.read_excel('nature12172-s1/Supplementary_Table4.xls', 'splicingTable.txt', index_col=(0,1))\n",
    "splicing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splicing = splicing.T\n",
    "splicing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The three pooled samples aren't named consistently with the expression data, so we have to fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splicing.index[splicing.index.map(lambda x: 'P' in x)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Since the pooled sample IDs are inconsistent with the `expression` data, we have to change them. We can get the \"P\" and the number after that using regular expressions, called ``re`` in the Python standard library, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "re.search(r'P\\d', '10,000 cell Rep1 (P1)').group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def long_pooled_name_to_short(x):\n",
    "    if 'P' not in x:\n",
    "        return x\n",
    "    else:\n",
    "        return re.search(r'P\\d', x).group()\n",
    "\n",
    "\n",
    "splicing.index.map(long_pooled_name_to_short)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "And now we assign this new index as our index to the ``splicing`` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splicing.index = splicing.index.map(long_pooled_name_to_short)\n",
    "splicing.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Right now, ``flotilla`` doesn't work with multi-indexed dataframes, so to deal with this, we'll use ``droplevel`` from ``pandas`` which will remove the gene name \"level\" from the column names. This is because the ``chr10:.....``-type thing is a unique identifier for splicing events, while the gene name is not (one gene may have many splicing events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splicing.columns = splicing.columns.droplevel(1)\n",
    "splicing.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Great, now we have a perfectly formatted splicing dataframe!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now let's get into creating a metadata dataframe. We'll use the index from the ``expression_filtered`` data to create the minimum required column, ``'phenotype'``, which has the name of the phenotype of that cell. And we'll also add the column ``'pooled'`` to indicate whether this sample is pooled or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metadata = pd.DataFrame(index=expression_filtered.index)\n",
    "metadata['phenotype'] = 'BDMC'\n",
    "metadata['pooled'] = metadata.index.map(lambda x: x.startswith('P'))\n",
    "\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping stats data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We also want to include the information about how well each sample's reads mapped to the genome, which the authors included as ``Supplementary_Table1.xls``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mapping_stats = pd.read_excel('nature12172-s1/Supplementary_Table1.xls', sheetname='SuppTable1 2.txt')\n",
    "mapping_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a `flotilla` Study!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We've now prepared five (5!) different dataframes of information that together represent this body of work. They are:\n",
    "\n",
    "* ``metadata``: A table describing each individual sample\n",
    "* ``expression_filtered``: A big matrix of gene expression values for each sample\n",
    "* ``expression_feature_data``: A table describing metadata on the genes, e.g. the category of the gene\n",
    "* ``splicing_data``: A big matrix of \"percent spliced-in\" splicing scores for each sample\n",
    "* ``mapping_stats_data``: A table describing the percentage of reads that mapped to the genome, plus other information about where the genes mapped, for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "study = flotilla.Study(# The metadata describing phenotype and pooled samples\n",
    "                       metadata, \n",
    "                       \n",
    "                       # A version for this data\n",
    "                       version='0.1.0', \n",
    "                       \n",
    "                       # Dataframe of the filtered expression data\n",
    "                       expression_data=expression_filtered,\n",
    "                       \n",
    "                       # Dataframe of the feature data of the genes\n",
    "                       expression_feature_data=expression_feature_data,\n",
    "                       \n",
    "                       # Dataframe of the splicing data\n",
    "                       splicing_data=splicing, \n",
    "                       \n",
    "                       # Dataframe of the mapping stats data\n",
    "                       mapping_stats_data=mapping_stats, \n",
    "                       \n",
    "                       # Which column in \"mapping_stats\" has the number of reads\n",
    "                       mapping_stats_number_mapped_col='PF_READS')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As a side note, you can save this study to disk now, so you can \"``embark``\" (load it) later:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the study so you can load it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "study.save('shalek2013')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note that this is saved to my home directory, in ``~/flotilla_projects/<study_name>/``. This will be saved in your home directory, too.\n",
    "\n",
    "The ``datapackage.json`` file is what holds all the information relative to the study, and loosely follows the `datapackage spec <http://data.okfn.org/doc/data-package>`_ created by the Open Knowledge Foundation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat /Users/olga/flotilla_projects/shalek2013/datapackage.json"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "One thing to note is that when you save, the version number is bumped up. ``study.version`` (the one we just made) is ``0.1.0``, but the one we saved is ``0.1.1``, since we could have made some changes to the data.\n",
    "\n",
    "Let's look at what else is in this folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls /Users/olga/flotilla_projects/shalek2013"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So this is where all the other files are. Good to know!\n",
    "\n",
    "We can \"embark\" on this newly-saved study now very painlessly, without having to open and process all those files again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "study2 = flotilla.embark('shalek2013')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
